Answer Sentence Selection (YodaQA)
==================================

In Question Answering systems on top of unstructured corpora, one task is
selecting the sentences in corpora that are most likely to carry an answer
to a given question.  In this scenario, the questions come from the
``curated-train`` YodaQA dataset and the YodaQA system generated the
candidate sentences based on enwiki, using YodaQA f/sentence-selection
branch commit a5f7f98 (based on v1.5).

Sentences were generated by running fulltext solr search on enwiki for
keywords extracted from the question, and then considering all sentences
from top N results that contain at least a single such keyword.  Sentences
that match the gold standard answer regex are labelled as 1, the rest is 0.
This *automatic labelling* means the dataset is **quite noisy**.

Several additional features are included:

  * **kwweights** - sum of weights of *non-about* qtext keywords matched in atext
  * **aboutkwweights** - sum of weights of *about* qtext keywords matched in atext,
    that is keywords that also matched the title of the source document
    (the hypothesis being that matching these yields less information)
  * **toklabels** - per-token labels of whether this label is part of the answer

The key metric here is MRR when ranking sentences answering the same question
by their score, but raw accuracy may be also interesting.  The dataset is
heavily unbalanced!  In our models, we subsample 0-entries as well as
supersample 1-entries.  First 80% entries result in the training set, last 20%
is validation set.  The test set is generated from the ``curated-test`` set.

This dataset is subject to change and evolution, hence the ``v2``.

Moreover, an extended dataset which is probably even more noisy (as the
questions haven't been so carefully curated, and gold standard regexes not
so carefully reviewed either) but about 3-4 times bigger is available as
``large2470``.  The splits of this dataset are supersets of the ``curated``
dataset.

TODO: Figure out whether it's better to report final results on the large2470
or curated test split.

Model Comparison
----------------

For randomized models, 95% confidence intervals (t-distribution) are reported.

The f_add_kw versions are ensembles of raw neural networks with yodaqakw scores.
We'll also shortly report BM25 ensemble results.  The new way to write this option
is ``f_add=['kw', 'akw']``.

curatedv2:

| Model                    | trainAllMRR | devMRR   | testMAP  | testMRR  | settings
|--------------------------|-------------|----------|----------|----------|---------
| yodaqakw                 | 0.368773    | 0.337348 | 0.284100 | 0.383238 | (defaults)
| termfreq TF-IDF #w       | 0.339544    | 0.324693 | 0.242700 | 0.337893 | ``freq_mode='tf'``
| termfreq BM25 #w         | 0.483538    | 0.452647 | 0.294300 | 0.484530 | (defaults)
|--------------------------|-------------|----------|----------|----------|---------
| avg                      | 0.422881    | 0.402618 | 0.229694 | 0.329356 | (defaults)
|                          |±0.024685    |±0.006664 |±0.001715 |±0.003511 |
| DAN                      | 0.437119    | 0.430754 | 0.233000 | 0.354075 | ``inp_e_dropout=0`` ``inp_w_dropout=1/3`` ``deep=2`` ``pact='relu'``
|                          |±0.014494    |±0.014477 |±0.002657 |±0.010307 |
|--------------------------|-------------|----------|----------|----------|---------
| rnn                      | 0.459869    | 0.429780 | 0.228869 | 0.341706 | (defaults)
|                          |±0.035981    |±0.015609 |±0.005554 |±0.010643 |
| cnn                      | 0.722432    | 0.395688 | 0.285637 | 0.391880 |
|                          |±0.190996    |±0.010156 |±0.004525 |±0.013766 |
| rnncnn                   | 0.578608    | 0.374195 | 0.238200 | 0.344659 | (defaults)
|                          |±0.044228    |±0.023533 |±0.007741 |±0.014747 |
| attn1511                 | 0.432403    | 0.475125 | 0.275219 | 0.468555 | (defaults)
|                          |±0.016183    |±0.012810 |±0.006562 |±0.014433 |
|--------------------------|-------------|----------|----------|----------|---------
| rnn                      | 0.600532    | 0.493167 | 0.300700 | 0.463808 | Ubuntu transfer learning (``ptscorer=B.dot_ptscorer`` ``pdim=1`` ``inp_e_dropout=0`` ``dropout=0`` ``balance_class=True`` ``adapt_ubuntu=True`` ``vocabt='ubuntu'`` ``opt='rmsprop'``)
|                          |±0.045585    |±0.015647 |±0.007871 |±0.011789 |
|--------------------------|-------------|----------|----------|----------|---------
| avg + BM25               | 0.523084    | 0.488757 | 0.258400 | 0.462038 |
|                          |±0.016611    |±0.006357 |±0.003675 |±0.010558 |
| DAN + BM25               | 0.532890    | 0.505769 | 0.269900 | 0.486230 | ``inp_e_dropout=0`` ``inp_w_dropout=1/3`` ``deep=2`` ``pact='relu'``
|                          |±0.022923    |±0.018970 |±0.006122 |±0.015990 |
| rnn + BM25               | 0.499628    | 0.486556 | 0.259022 | 0.466565 |
|                          |±0.007046    |±0.007320 |±0.002541 |±0.005959 |
| cnn + BM25               | 0.588734    | 0.529302 | 0.271288 | 0.487668 |
|                          |±0.028836    |±0.015761 |±0.003108 |±0.009528 |
| rnncnn + BM25            | 0.534710    | 0.478538 | 0.266200 | 0.473099 |
|                          |±0.017699    |±0.010704 |±0.003659 |±0.011603 |
| attn1511 + BM25          | 0.486697    | 0.462924 | 0.271363 | 0.488915 |
|                          |±0.012825    |±0.007982 |±0.001888 |±0.005102 |

These results are obtained like this:

	tools/train.py avg anssel data/anssel/yodaqa/curatedv2-training.csv data/anssel/yodaqa/curatedv2-val.csv nb_runs=16
	tools/eval.py avg anssel data/anssel/yodaqa/curatedv2-training.csv data/anssel/yodaqa/curatedv2-val.csv data/anssel/yodaqa/curatedv2-test.csv weights-anssel-avg--69489c8dc3b6ce11-*-bestval.h5

large2470:

| Model                    | trainAllMRR | devMRR   | testMAP  | testMRR  | settings
|--------------------------|-------------|----------|----------|----------|---------
| yodaqakw                 | 0.332693    | 0.318246 | 0.303900 | 0.376465 | (defaults)
| termfreq TF-IDF #w       | 0.325390    | 0.328255 | 0.266800 | 0.362613 | ``freq_mode='tf'``
| termfreq BM25 #w         | 0.441573    | 0.432115 | 0.313900 | 0.490822 | (defaults)
|--------------------------|-------------|----------|----------|----------|---------
| avg                      | 0.798883    | 0.408034 | 0.262569 | 0.362190 | (defaults)
|                          |±0.026554    |±0.004656 |±0.002054 |±0.005725 |
| DAN                      | 0.646481    | 0.404210 | 0.272675 | 0.386522 | ``inp_e_dropout=0`` ``inp_w_dropout=1/3`` ``deep=2`` ``pact='relu'``
|                          |±0.070994    |±0.005378 |±0.003028 |±0.007627 |
|--------------------------|-------------|----------|----------|----------|---------
| rnn                      | 0.460984    | 0.382949 | 0.262463 | 0.381298 | (defaults)
|                          |±0.023715    |±0.006451 |±0.002641 |±0.007643 |
| cnn                      | 0.550441    | 0.348247 | 0.264476 | 0.353243 | ``inp_e_dropout=1/2`` ``dropout=1/2`` (FIXME)
|                          |±0.069701    |±0.006217 |±0.002918 |±0.009620 |
| rnncnn                   | 0.681908    | 0.408662 | 0.286118 | 0.394865 | (defaults)
|                          |±0.114967    |±0.008659 |±0.003501 |±0.011895 |
| attn1511                 | 0.445635    | 0.408495 | 0.288100 | 0.430892 | (defaults)
|                          |±0.056352    |±0.008744 |±0.005601 |±0.017858 |
|--------------------------|-------------|----------|----------|----------|---------
| rnn                      | 0.623209    | 0.517763 | 0.359331 | 0.539284 | Ubuntu transfer learning (``ptscorer=B.dot_ptscorer`` ``pdim=1`` ``inp_e_dropout=0`` ``dropout=0`` ``balance_class=True`` ``adapt_ubuntu=True`` ``opt='rmsprop'``)
|                          |±0.014351    |±0.007724 |±0.003003 |±0.005755 |
|--------------------------|-------------|----------|----------|----------|---------
| avg + BM25               | 0.587060    | 0.461915 | 0.278288 | 0.480614 |
|                          |±0.031841    |±0.004170 |±0.002869 |±0.007834 |
| DAN + BM25               | 0.550746    | 0.466123 | 0.281512 | 0.490113 | ``inp_e_dropout=0`` ``inp_w_dropout=1/3`` ``deep=2`` ``pact='relu'``
|                          |±0.023915    |±0.003946 |±0.004459 |±0.010337 |
| rnn + BM25               | 0.496494    | 0.456415 | 0.276863 | 0.486928 |
|                          |±0.015167    |±0.007189 |±0.003576 |±0.008479 |
| cnn + BM25               | 0.616683    | 0.480827 | 0.287931 | 0.498852 |
|                          |±0.015484    |±0.004605 |±0.002607 |±0.006762 |
| rnncnn + BM25            | 0.569165    | 0.479459 | 0.287856 | 0.503145 |
|                          |±0.021074    |±0.004721 |±0.003550 |±0.009965 |
| attn1511 + BM25          | 0.531593    | 0.462170 | 0.286069 | 0.499340 |
|                          |±0.039245    |±0.006922 |±0.003301 |±0.008598 |

These results are obtained like this:

	tools/train.py avg anssel data/anssel/yodaqa/large2470-training.csv data/anssel/yodaqa/large2470-val.csv nb_runs=16
	tools/eval.py avg anssel data/anssel/yodaqa/large2470-training.csv data/anssel/yodaqa/large2470-val.csv data/anssel/yodaqa/large2470-test.csv weights-anssel-avg--69489c8dc3b6ce11-*-bestval.h5

BM25 results are obtained with:

	"prescoring='termfreq'" "prescoring_weightsf='weights-anssel-termfreq-3368350fbcab42e4-bestval.h5'" "prescoring_input='bm25'" "f_add=['bm25']" prescoring_prune=20

Transfer learning has been performed like this:

	tools/transfer.py rnn ubuntu data/anssel/ubuntu/v2-vocab.pickle ubu-weights-rnn--23fa2eff7cda310d-bestval.h5 anssel data/anssel/yodaqa/curatedv2-training.csv data/anssel/yodaqa/curatedv2-val.csv pdim=1 ptscorer=B.dot_ptscorer dropout=0 inp_e_dropout=0 balance_class=True adapt_ubuntu=True "opt='rmsprop'" nb_runs=16
	tools/eval.py rnn anssel data/anssel/yodaqa/curatedv2-training.csv data/anssel/yodaqa/curatedv2-val.csv data/anssel/yodaqa/curatedv2-test.csv weights-ubuntu-anssel-rnn-1cd9ebbf9c99f926-*-bestval.h5 "vocabf='data/anssel/ubuntu/v2-vocab.pickle'" "vocabt='ubuntu'" pdim=1 ptscorer=B.dot_ptscorer inp_e_dropout=0 dropout=0 balance_class=True adapt_ubuntu=True "opt='rmsprop'"

where the ubu-weights model can be downloaded from

	http://pasky.or.cz/dev/brmson/ubu-weights-rnn--23fa2eff7cda310d-bestval.h5


Older Datasets
--------------

The academic standard up to now stems from **anssel/wang** - the TREC-based
dataset originally by Wang et al., 2007, in the form by Yao et al., 2013.
However, this dataset seems to be quite easy as the ratio of relevant snippets
is high and they are often pretty short.

That's why we are introducing a new one, which is also somewhat bigger.

See also the WikiQA corpus, which has too restrictive licence but manually
labelled pairs.

Licence
-------

Derived from the ``curated-train`` YodaQA dataset

	htts://github.com/brmson/dataset-factoid-curated

and sentences from Wikipedia, Wikipedia is CC-BY-SA if that's relevant.
